{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atsushi.hara/.anyenv/envs/pyenv/versions/anaconda3-5.0.1/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/atsushi.hara/.anyenv/envs/pyenv/versions/anaconda3-5.0.1/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'temp'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "cifar10_url = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "\n",
    "data_file = os.path.join(data_dir, 'cifar-10-binary.tar.gz')\n",
    "\n",
    "if os.path.isfile(data_file):\n",
    "    pass\n",
    "else:\n",
    "    def progress(block_num, block_size, total_size):\n",
    "        progress_info = [cifar10_url, float(block_num * block_size) / float(total_size) * 100]\n",
    "        print('\\r Downloading {} - {:.2f}%'.format(*progress_info), end=\"\")\n",
    "    filepath, _ = urllib.request.urlretrieve(cifar10_url, data_file, progress)\n",
    "    tarfile.open(filepath, 'r:gz').extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "output_every = 50\n",
    "generations = 20000\n",
    "eval_every = 500\n",
    "image_height = 32\n",
    "image_width = 32\n",
    "crop_height = 24\n",
    "crop_width = 24\n",
    "num_channels = 3\n",
    "num_targets = 10\n",
    "extract_folder = 'cifar-10-batches-bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Learning Rate Decay Params\n",
    "learning_rate = 0.1\n",
    "lr_decay = 0.1\n",
    "num_gens_to_wait = 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vec_length = image_height * image_width * num_channels\n",
    "record_length = 1 + image_vec_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cifar_files(filename_queue, distort_images=True):\n",
    "    reader = tf.FixedLengthRecordReader(record_bytes=record_length)\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    record_bytes = tf.decode_raw(record_string, tf.uint8)\n",
    "    \n",
    "    image_label = tf.cast(tf.slice(record_bytes, [0], [1]), tf.int32)\n",
    "    \n",
    "    image_extracted = tf.reshape(tf.slice(record_bytes, [1], [image_vec_length]), [num_channels, image_height, image_width])\n",
    "    \n",
    "    image_uint8image = tf.transpose(image_extracted, [1, 2, 0])\n",
    "    reshaped_image = tf.cast(image_uint8image, tf.float32)\n",
    "    \n",
    "    final_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, crop_width, crop_height)\n",
    "    \n",
    "    if distort_images:\n",
    "        final_image = tf.image.random_flip_left_right(final_image)\n",
    "        final_image = tf.image.random_brightness(final_image, max_delta=63)\n",
    "        final_image = tf.image.random_contrast(final_image, lower=0.2, upper=1.8)\n",
    "        \n",
    "    final_image = tf.image.per_image_standardization(final_image)\n",
    "    return (final_image, image_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pipline(batch_size, train_logical=True):\n",
    "    if train_logical:\n",
    "        files = [os.path.join(data_dir, extract_folder, 'data_batch_{}.bin'.format(i)) for i in range(1, 6)]\n",
    "    else:\n",
    "        files = [os.path.join(data_dir, extract_folder, 'test_batch.bin')]\n",
    "        \n",
    "    filename_queue = tf.train.string_input_producer(files)\n",
    "    image, label = read_cifar_files(filename_queue)\n",
    "    \n",
    "    # In tensorflow document, recommended size: \n",
    "    # min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 5000 \n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        capacity=capacity,\n",
    "        min_after_dequeue=min_after_dequeue)\n",
    "    \n",
    "    return (example_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_cnn_model(input_images, batch_size, train_logical=True):\n",
    "    def truncated_normal_var(name, shape, dtype):\n",
    "        return (tf.get_variable(name=name, shape=shape, dtype=dtype,\n",
    "                               initializer=tf.truncated_normal_initializer(stddev=0.05)))\n",
    "    \n",
    "    def zero_var(name, shape, dtype):\n",
    "        return (tf.get_variable(name=name, shape=shape, dtype=dtype,\n",
    "                               initializer=tf.constant_initializer(0.0)))\n",
    "    \n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        conv1_kernel = truncated_normal_var(name='conv_kernel1',\n",
    "                                            shape=[5, 5, 3, 64],\n",
    "                                            dtype=tf.float32)\n",
    "        \n",
    "        conv1 = tf.nn.conv2d(input_images, conv1_kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        conv1_bias = zero_var(name='conv_bias1', shape=[64], dtype=tf.float32)\n",
    "        \n",
    "        conv1_add_bias = tf.nn.bias_add(conv1, conv1_bias)\n",
    "        relu_conv1 = tf.nn.relu(conv1_add_bias)\n",
    "        \n",
    "    pool1 = tf.nn.max_pool(relu_conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_layer1')\n",
    "    norm1 = tf.nn.lrn(pool1, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name='norm1')\n",
    "    \n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        conv2_kernel = truncated_normal_var(name='conv_kernel2',\n",
    "                                            shape=[5, 5, 64, 64],\n",
    "                                            dtype=tf.float32)\n",
    "        \n",
    "        conv2 = tf.nn.conv2d(norm1, conv2_kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        conv2_bias = zero_var(name='conv_bias2', shape=[64], dtype=tf.float32)\n",
    "        \n",
    "        conv2_add_bias = tf.nn.bias_add(conv2, conv2_bias)\n",
    "        relu_conv2 = tf.nn.relu(conv2_add_bias)\n",
    "        \n",
    "    pool2 = tf.nn.max_pool(relu_conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_layer2')\n",
    "    norm2 = tf.nn.lrn(pool2, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name='norm2')\n",
    "    \n",
    "    reshaped_output = tf.reshape(norm2, [batch_size, -1])\n",
    "    reshaped_dim = reshaped_output.get_shape()[1].value\n",
    "    \n",
    "    with tf.variable_scope('full1') as scope:\n",
    "        full_weight1 = truncated_normal_var(name='full_mult1', shape=[reshaped_dim, 384], dtype=tf.float32)\n",
    "        full_bias1 = zero_var(name='full_bias1', shape=[384], dtype=tf.float32)\n",
    "        full_layer1 = tf.nn.relu(tf.add(tf.matmul(reshaped_output, full_weight1), full_bias1))\n",
    "        \n",
    "    with tf.variable_scope('full2') as scope:\n",
    "        full_weight2 = truncated_normal_var(name='full_mult2', shape=[384, 192], dtype=tf.float32)\n",
    "        full_bias2 = zero_var(name='full_bias2', shape=[192], dtype=tf.float32)\n",
    "        full_layer2 = tf.nn.relu(tf.add(tf.matmul(full_layer1, full_weight2), full_bias2))\n",
    "        \n",
    "    with tf.variable_scope('full3') as scope:\n",
    "        full_weight3 = truncated_normal_var(name='full_mult3', shape=[192, num_targets], dtype=tf.float32)\n",
    "        full_bias3 = zero_var(name='full_bias3', shape=[num_targets], dtype=tf.float32)\n",
    "        final_output = tf.add(tf.matmul(full_layer2, full_weight3), full_bias3)\n",
    "        \n",
    "    return (final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_loss(logits, targets):\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    return (cross_entropy_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(loss_value, generation_num):\n",
    "    model_learning_rate = tf.train.exponential_decay(learning_rate, generation_num, num_gens_to_wait, lr_decay, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(model_learning_rate)\n",
    "    train_step = optimizer.minimize(loss_value)\n",
    "    \n",
    "    return (train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_of_batch(logits, targets):\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    batch_pred = tf.cast(tf.argmax(logits, 1), tf.int32)\n",
    "    predict_correctly = tf.equal(batch_pred, targets)\n",
    "    accuracy = tf.reduce_mean(tf.cast(predict_correctly, tf.float32))\n",
    "    return (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = input_pipline(batch_size, train_logical=True)\n",
    "test_images, test_targets = input_pipline(batch_size, train_logical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('model_definition') as scope:\n",
    "    model_output = cifar_cnn_model(images, batch_size)\n",
    "    scope.reuse_variables()\n",
    "    test_output = cifar_cnn_model(test_images, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cifar_loss(model_output, targets)\n",
    "accuracy = accuracy_of_batch(test_output, test_targets)\n",
    "\n",
    "generation_num = tf.Variable(0, trainable=False)\n",
    "train_op = train_step(loss, generation_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(QueueRunnerThread-input_producer-input_producer/input_producer_EnqueueMany, started daemon 123145539432448)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145544687616)>,\n",
       " <Thread(QueueRunnerThread-input_producer_1-input_producer_1/input_producer_1_EnqueueMany, started daemon 123145549942784)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch_1/random_shuffle_queue-shuffle_batch_1/random_shuffle_queue_enqueue, started daemon 123145555197952)>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.start_queue_runners(sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 50: Loss = 2.31603\n",
      "Generation 100: Loss = 1.84011\n",
      "Generation 150: Loss = 2.04907\n",
      "Generation 200: Loss = 1.80914\n",
      "Generation 250: Loss = 1.48046\n",
      "Generation 300: Loss = 1.59522\n",
      "Generation 350: Loss = 1.42921\n",
      "Generation 400: Loss = 1.43132\n",
      "Generation 450: Loss = 1.48466\n",
      "Generation 500: Loss = 1.40051\n",
      " --- Test Accuracy= 53.12%.\n",
      "Generation 550: Loss = 1.34574\n",
      "Generation 600: Loss = 1.33773\n",
      "Generation 650: Loss = 1.32544\n",
      "Generation 700: Loss = 1.28119\n",
      "Generation 750: Loss = 1.23681\n",
      "Generation 800: Loss = 1.26085\n",
      "Generation 850: Loss = 1.24804\n",
      "Generation 900: Loss = 1.12132\n",
      "Generation 950: Loss = 1.14938\n",
      "Generation 1000: Loss = 1.37919\n",
      " --- Test Accuracy= 66.41%.\n",
      "Generation 1050: Loss = 1.00659\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "for i in range(generations):\n",
    "    _, loss_value = sess.run([train_op, loss])\n",
    "    \n",
    "    if (i+1) % output_every == 0:\n",
    "        train_loss.append(loss_value)\n",
    "        output = 'Generation {}: Loss = {:.5f}'.format((i+1), loss_value)\n",
    "        print(output)\n",
    "        \n",
    "    if (i+1) % eval_every == 0:\n",
    "        [temp_accuracy] = sess.run([accuracy])\n",
    "        test_accuracy.append(temp_accuracy)\n",
    "        acc_output = ' --- Test Accuracy= {:.2f}%.'.format(100.*temp_accuracy)\n",
    "        print(acc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_indices = range(0, generations, eval_every)\n",
    "output_indices = range(0, generations, output_every)\n",
    "\n",
    "plt.plot(output_indices, train_loss, 'k-')\n",
    "plt.title('Softmax Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Softmax Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(eval_indices, test_accuracy, 'k-')\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
