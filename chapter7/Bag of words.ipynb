{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_file_name = os.path.join('temp', 'temp_spam_data.csv')\n",
    "\n",
    "if not os.path.exists('temp'):\n",
    "    os.makedirs('temp')\n",
    "    \n",
    "if os.path.isfile(save_file_name):\n",
    "    text_data = []\n",
    "    with open(save_file_name, 'r') as temp_output_file:\n",
    "        reader = csv.reader(temp_output_file)\n",
    "        for row in reader:\n",
    "            text_data.append(row)\n",
    "else:\n",
    "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/' \\\n",
    "              '00228/smsspamcollection.zip'\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('SMSSpamCollection')\n",
    "    \n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode('ascii', errors='ignore')\n",
    "    text_data = text_data.decode().split('\\n')\n",
    "    text_data = [x.split('\\t') for x in text_data if len(x) >= 1]\n",
    "    \n",
    "    with open(save_file_name, 'w') as temp_output_file:\n",
    "        writer = csv.writer(temp_output_file)\n",
    "        writer.writerows(text_data)\n",
    "        \n",
    "texts = [x[1] for x in text_data]\n",
    "targets = [x[0] for x in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ham',\n",
       "  'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'],\n",
       " ['ham', 'Ok lar... Joking wif u oni...']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       " 'Ok lar... Joking wif u oni...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham', 'ham']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change spam's label to 1, ham's label to 0\n",
    "target = [1 if x == 'spam' else 0 for x in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalize text\n",
    "texts = [x.lower() for x in texts]\n",
    "texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "texts = [' '.join(x.split()) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat',\n",
       " 'ok lar joking wif u oni']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12640fc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_lengths = [len(x.split()) for x in texts]\n",
    "text_lengths = [x for x in text_lengths if x < 50]\n",
    "\n",
    "plt.hist(text_lengths, bins=25)\n",
    "plt.title('Histogram of # of Words in Texts')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_size = 40\n",
    "min_word_freq = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_processor = learn.preprocessing.VocabularyProcessor(\n",
    "    sentence_size, min_frequency=min_word_freq)\n",
    "\n",
    "vocab_processor.fit_transform(texts)\n",
    "embedding_size = len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2108"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_indices = np.random.choice(len(texts), round(len(texts)*0.8), replace=False)\n",
    "test_indices = np.array(list(set(range(len(texts))) - set(train_indices)))\n",
    "\n",
    "texts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\n",
    "texts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\n",
    "target_train = [x for ix, x in enumerate(target) if ix in train_indices]\n",
    "target_test = [x for ix, x in enumerate(target) if ix in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "identity_mat = tf.diag(tf.ones(shape=[embedding_size,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Diag:0' shape=(2108, 2108) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables for logistic regression\n",
    "A = tf.Variable(tf.random_normal(shape=[embedding_size, 1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders for input data\n",
    "x_data = tf.placeholder(shape=[sentence_size], dtype=tf.int32)\n",
    "y_target = tf.placeholder(shape=[1,1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_embed = tf.nn.embedding_lookup(identity_mat, x_data)\n",
    "x_col_sums = tf.reduce_sum(x_embed, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_col_sums_2D = tf.expand_dims(x_col_sums, 0)\n",
    "model_output = tf.add(tf.matmul(x_col_sums_2D, A), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, targets=y_target))\n",
    "predict = tf.sigmoid(model_output)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "train_acc = []\n",
    "train_acc_ave = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Observation #10: loss = 7.23739\n",
      "Train Observation #20: loss = 0.182008\n",
      "Train Observation #30: loss = 0.000897706\n",
      "Train Observation #40: loss = 0.166723\n",
      "Train Observation #50: loss = 9.55334e-06\n",
      "Train Observation #60: loss = 0.308716\n",
      "Train Observation #70: loss = 0.199991\n",
      "Train Observation #80: loss = 2.52861e-06\n",
      "Train Observation #90: loss = 1.11846\n",
      "Train Observation #100: loss = 0.000600166\n",
      "Train Observation #110: loss = 0.0239316\n",
      "Train Observation #120: loss = 0.0321256\n",
      "Train Observation #130: loss = 3.84802\n",
      "Train Observation #140: loss = 0.836534\n",
      "Train Observation #150: loss = 0.00141158\n",
      "Train Observation #160: loss = 0.0132421\n",
      "Train Observation #170: loss = 0.00445867\n",
      "Train Observation #180: loss = 7.00551\n",
      "Train Observation #190: loss = 0.000543977\n",
      "Train Observation #200: loss = 7.76219\n",
      "Train Observation #210: loss = 4.57832\n",
      "Train Observation #220: loss = 0.00242458\n",
      "Train Observation #230: loss = 2.77876e-05\n",
      "Train Observation #240: loss = 0.0108576\n",
      "Train Observation #250: loss = 5.28961\n",
      "Train Observation #260: loss = 1.87399e-05\n",
      "Train Observation #270: loss = 0.228624\n",
      "Train Observation #280: loss = 4.41485\n",
      "Train Observation #290: loss = 0.031476\n",
      "Train Observation #300: loss = 0.130672\n",
      "Train Observation #310: loss = 0.344797\n",
      "Train Observation #320: loss = 0.0466174\n",
      "Train Observation #330: loss = 0.0503934\n",
      "Train Observation #340: loss = 1.20733\n",
      "Train Observation #350: loss = 0.0228897\n",
      "Train Observation #360: loss = 0.00385203\n",
      "Train Observation #370: loss = 0.00141542\n",
      "Train Observation #380: loss = 0.00520745\n",
      "Train Observation #390: loss = 3.08741\n",
      "Train Observation #400: loss = 13.6822\n",
      "Train Observation #410: loss = 0.0137834\n",
      "Train Observation #420: loss = 0.513583\n",
      "Train Observation #430: loss = 1.5263\n",
      "Train Observation #440: loss = 0.0954998\n",
      "Train Observation #450: loss = 0.000443936\n",
      "Train Observation #460: loss = 0.00212722\n",
      "Train Observation #470: loss = 0.00704008\n",
      "Train Observation #480: loss = 0.056144\n",
      "Train Observation #490: loss = 10.3164\n",
      "Train Observation #500: loss = 0.0870277\n",
      "Train Observation #510: loss = 0.0953922\n",
      "Train Observation #520: loss = 8.09874e-06\n",
      "Train Observation #530: loss = 8.98292e-05\n",
      "Train Observation #540: loss = 0.00239194\n",
      "Train Observation #550: loss = 9.19463\n",
      "Train Observation #560: loss = 1.11629\n",
      "Train Observation #570: loss = 3.33258\n",
      "Train Observation #580: loss = 0.215895\n",
      "Train Observation #590: loss = 0.0101204\n",
      "Train Observation #600: loss = 0.0130541\n",
      "Train Observation #610: loss = 0.0459004\n",
      "Train Observation #620: loss = 0.114351\n",
      "Train Observation #630: loss = 0.0236694\n",
      "Train Observation #640: loss = 0.0571838\n",
      "Train Observation #650: loss = 0.0241737\n",
      "Train Observation #660: loss = 0.0337747\n",
      "Train Observation #670: loss = 0.198427\n",
      "Train Observation #680: loss = 0.00194174\n",
      "Train Observation #690: loss = 0.00442261\n",
      "Train Observation #700: loss = 2.70465\n",
      "Train Observation #710: loss = 0.465672\n",
      "Train Observation #720: loss = 3.71123\n",
      "Train Observation #730: loss = 0.0014399\n",
      "Train Observation #740: loss = 3.34145\n",
      "Train Observation #750: loss = 0.0398885\n",
      "Train Observation #760: loss = 0.00056534\n",
      "Train Observation #770: loss = 6.50436\n",
      "Train Observation #780: loss = 0.367782\n",
      "Train Observation #790: loss = 0.0206359\n",
      "Train Observation #800: loss = 0.0127466\n",
      "Train Observation #810: loss = 1.29862\n",
      "Train Observation #820: loss = 0.000627308\n",
      "Train Observation #830: loss = 0.0235652\n",
      "Train Observation #840: loss = 0.016183\n",
      "Train Observation #850: loss = 0.00179081\n",
      "Train Observation #860: loss = 4.18686\n",
      "Train Observation #870: loss = 0.0397143\n",
      "Train Observation #880: loss = 0.00228966\n",
      "Train Observation #890: loss = 6.44931e-07\n",
      "Train Observation #900: loss = 0.0217158\n",
      "Train Observation #910: loss = 0.399416\n",
      "Train Observation #920: loss = 4.45359e-07\n",
      "Train Observation #930: loss = 0.000556041\n",
      "Train Observation #940: loss = 6.79615\n",
      "Train Observation #950: loss = 0.000873108\n",
      "Train Observation #960: loss = 7.63088\n",
      "Train Observation #970: loss = 21.9899\n",
      "Train Observation #980: loss = 6.74416\n",
      "Train Observation #990: loss = 5.81293\n",
      "Train Observation #1000: loss = 0.00347783\n",
      "Train Observation #1010: loss = 1.30098e-05\n",
      "Train Observation #1020: loss = 0.0416806\n",
      "Train Observation #1030: loss = 0.151594\n",
      "Train Observation #1040: loss = 0.0598767\n",
      "Train Observation #1050: loss = 6.14611e-05\n",
      "Train Observation #1060: loss = 6.97874\n",
      "Train Observation #1070: loss = 1.43106e-05\n",
      "Train Observation #1080: loss = 0.000874691\n",
      "Train Observation #1090: loss = 0.00186546\n",
      "Train Observation #1100: loss = 4.34294\n",
      "Train Observation #1110: loss = 1.01218\n",
      "Train Observation #1120: loss = 0.00453846\n",
      "Train Observation #1130: loss = 0.00185847\n",
      "Train Observation #1140: loss = 5.52309e-05\n",
      "Train Observation #1150: loss = 0.00274323\n",
      "Train Observation #1160: loss = 0.028275\n",
      "Train Observation #1170: loss = 0.000848292\n",
      "Train Observation #1180: loss = 1.50857\n",
      "Train Observation #1190: loss = 0.0683144\n",
      "Train Observation #1200: loss = 0.417438\n",
      "Train Observation #1210: loss = 0.000310874\n",
      "Train Observation #1220: loss = 0.000697511\n",
      "Train Observation #1230: loss = 4.4914\n",
      "Train Observation #1240: loss = 0.0362614\n",
      "Train Observation #1250: loss = 9.93252e-05\n",
      "Train Observation #1260: loss = 4.55338\n",
      "Train Observation #1270: loss = 0.00127148\n",
      "Train Observation #1280: loss = 9.39726\n",
      "Train Observation #1290: loss = 0.00109359\n",
      "Train Observation #1300: loss = 2.69553e-07\n",
      "Train Observation #1310: loss = 0.000319096\n",
      "Train Observation #1320: loss = 1.26055e-05\n",
      "Train Observation #1330: loss = 0.213838\n",
      "Train Observation #1340: loss = 0.000100791\n",
      "Train Observation #1350: loss = 0.411739\n",
      "Train Observation #1360: loss = 0.0432281\n",
      "Train Observation #1370: loss = 1.51861\n",
      "Train Observation #1380: loss = 0.113441\n",
      "Train Observation #1390: loss = 0.00228565\n",
      "Train Observation #1400: loss = 0.366552\n",
      "Train Observation #1410: loss = 0.0160299\n",
      "Train Observation #1420: loss = 0.00242421\n",
      "Train Observation #1430: loss = 0.00195088\n",
      "Train Observation #1440: loss = 0.00223062\n",
      "Train Observation #1450: loss = 0.00108423\n",
      "Train Observation #1460: loss = 0.000200846\n",
      "Train Observation #1470: loss = 0.0962202\n",
      "Train Observation #1480: loss = 1.81332e-06\n",
      "Train Observation #1490: loss = 0.0713695\n",
      "Train Observation #1500: loss = 8.44325e-06\n",
      "Train Observation #1510: loss = 0.00419993\n",
      "Train Observation #1520: loss = 0.0141092\n",
      "Train Observation #1530: loss = 0.00649952\n",
      "Train Observation #1540: loss = 0.000217048\n",
      "Train Observation #1550: loss = 0.00436382\n",
      "Train Observation #1560: loss = 0.00052564\n",
      "Train Observation #1570: loss = 0.00571774\n",
      "Train Observation #1580: loss = 0.00347411\n",
      "Train Observation #1590: loss = 0.0079964\n",
      "Train Observation #1600: loss = 0.0018394\n",
      "Train Observation #1610: loss = 0.477693\n",
      "Train Observation #1620: loss = 9.56929e-06\n",
      "Train Observation #1630: loss = 0.054627\n",
      "Train Observation #1640: loss = 0.36672\n",
      "Train Observation #1650: loss = 0.0893135\n",
      "Train Observation #1660: loss = 0.00353232\n",
      "Train Observation #1670: loss = 1.62889\n",
      "Train Observation #1680: loss = 0.040281\n",
      "Train Observation #1690: loss = 0.000837738\n",
      "Train Observation #1700: loss = 0.00108724\n",
      "Train Observation #1710: loss = 0.295255\n",
      "Train Observation #1720: loss = 0.224216\n",
      "Train Observation #1730: loss = 0.100594\n",
      "Train Observation #1740: loss = 0.00321012\n",
      "Train Observation #1750: loss = 0.474839\n",
      "Train Observation #1760: loss = 0.000368412\n",
      "Train Observation #1770: loss = 0.00377712\n",
      "Train Observation #1780: loss = 0.000126651\n",
      "Train Observation #1790: loss = 3.62247\n",
      "Train Observation #1800: loss = 4.19267e-05\n",
      "Train Observation #1810: loss = 0.0286883\n",
      "Train Observation #1820: loss = 0.000599694\n",
      "Train Observation #1830: loss = 3.3698\n",
      "Train Observation #1840: loss = 0.149223\n",
      "Train Observation #1850: loss = 2.93707\n",
      "Train Observation #1860: loss = 0.294866\n",
      "Train Observation #1870: loss = 0.00233808\n",
      "Train Observation #1880: loss = 0.00336236\n",
      "Train Observation #1890: loss = 0.0012352\n",
      "Train Observation #1900: loss = 1.6067\n",
      "Train Observation #1910: loss = 0.000325507\n",
      "Train Observation #1920: loss = 0.00452177\n",
      "Train Observation #1930: loss = 0.0387279\n",
      "Train Observation #1940: loss = 5.63886\n",
      "Train Observation #1950: loss = 1.92949e-05\n",
      "Train Observation #1960: loss = 1.34921e-05\n",
      "Train Observation #1970: loss = 0.0885688\n",
      "Train Observation #1980: loss = 3.62939\n",
      "Train Observation #1990: loss = 0.00255477\n",
      "Train Observation #2000: loss = 0.0030878\n",
      "Train Observation #2010: loss = 0.0051316\n",
      "Train Observation #2020: loss = 4.25457e-06\n",
      "Train Observation #2030: loss = 9.21227\n",
      "Train Observation #2040: loss = 0.000150391\n",
      "Train Observation #2050: loss = 0.000582073\n",
      "Train Observation #2060: loss = 0.00421909\n",
      "Train Observation #2070: loss = 0.711029\n",
      "Train Observation #2080: loss = 0.163883\n",
      "Train Observation #2090: loss = 0.0547007\n",
      "Train Observation #2100: loss = 0.000683874\n",
      "Train Observation #2110: loss = 0.0228269\n",
      "Train Observation #2120: loss = 17.4586\n",
      "Train Observation #2130: loss = 1.77243e-06\n",
      "Train Observation #2140: loss = 11.7086\n",
      "Train Observation #2150: loss = 0.0987574\n",
      "Train Observation #2160: loss = 0.904191\n",
      "Train Observation #2170: loss = 0.322658\n",
      "Train Observation #2180: loss = 0.0651076\n",
      "Train Observation #2190: loss = 0.00336159\n",
      "Train Observation #2200: loss = 0.000410748\n",
      "Train Observation #2210: loss = 0.089717\n",
      "Train Observation #2220: loss = 0.0780388\n",
      "Train Observation #2230: loss = 0.0348018\n",
      "Train Observation #2240: loss = 0.00234556\n",
      "Train Observation #2250: loss = 2.41383\n",
      "Train Observation #2260: loss = 0.011146\n",
      "Train Observation #2270: loss = 7.35178e-05\n",
      "Train Observation #2280: loss = 0.000783729\n",
      "Train Observation #2290: loss = 0.00947978\n",
      "Train Observation #2300: loss = 0.275207\n",
      "Train Observation #2310: loss = 0.0246052\n",
      "Train Observation #2320: loss = 0.0482739\n",
      "Train Observation #2330: loss = 0.00109275\n",
      "Train Observation #2340: loss = 5.10241e-05\n",
      "Train Observation #2350: loss = 0.0159065\n",
      "Train Observation #2360: loss = 0.0171466\n",
      "Train Observation #2370: loss = 0.00255647\n",
      "Train Observation #2380: loss = 1.50937\n",
      "Train Observation #2390: loss = 2.84175\n",
      "Train Observation #2400: loss = 5.60647\n",
      "Train Observation #2410: loss = 0.00561082\n",
      "Train Observation #2420: loss = 0.000392801\n",
      "Train Observation #2430: loss = 4.54355e-05\n",
      "Train Observation #2440: loss = 0.0418502\n",
      "Train Observation #2450: loss = 0.0263591\n",
      "Train Observation #2460: loss = 0.000256805\n",
      "Train Observation #2470: loss = 0.00109122\n",
      "Train Observation #2480: loss = 9.15053e-05\n",
      "Train Observation #2490: loss = 0.0190067\n",
      "Train Observation #2500: loss = 0.00186708\n",
      "Train Observation #2510: loss = 0.00183972\n",
      "Train Observation #2520: loss = 0.0107959\n",
      "Train Observation #2530: loss = 3.28508\n",
      "Train Observation #2540: loss = 0.000963221\n",
      "Train Observation #2550: loss = 0.000235406\n",
      "Train Observation #2560: loss = 0.00537122\n",
      "Train Observation #2570: loss = 8.35833e-06\n",
      "Train Observation #2580: loss = 0.000524234\n",
      "Train Observation #2590: loss = 0.00262688\n",
      "Train Observation #2600: loss = 0.00253726\n",
      "Train Observation #2610: loss = 0.000644847\n",
      "Train Observation #2620: loss = 5.40995e-05\n",
      "Train Observation #2630: loss = 3.08426e-05\n",
      "Train Observation #2640: loss = 4.76702\n",
      "Train Observation #2650: loss = 1.25975e-05\n",
      "Train Observation #2660: loss = 1.5213e-06\n",
      "Train Observation #2670: loss = 7.42058e-06\n",
      "Train Observation #2680: loss = 0.023112\n",
      "Train Observation #2690: loss = 0.00134342\n",
      "Train Observation #2700: loss = 0.000689456\n",
      "Train Observation #2710: loss = 0.00258168\n",
      "Train Observation #2720: loss = 0.00141743\n",
      "Train Observation #2730: loss = 0.000542922\n",
      "Train Observation #2740: loss = 8.20226\n",
      "Train Observation #2750: loss = 2.63195\n",
      "Train Observation #2760: loss = 0.0762413\n",
      "Train Observation #2770: loss = 0.000491154\n",
      "Train Observation #2780: loss = 0.00430388\n",
      "Train Observation #2790: loss = 0.798188\n",
      "Train Observation #2800: loss = 9.55189\n",
      "Train Observation #2810: loss = 4.62342e-07\n",
      "Train Observation #2820: loss = 0.0404559\n",
      "Train Observation #2830: loss = 0.00875673\n",
      "Train Observation #2840: loss = 0.00435079\n",
      "Train Observation #2850: loss = 0.000108236\n",
      "Train Observation #2860: loss = 11.1072\n",
      "Train Observation #2870: loss = 3.33303e-05\n",
      "Train Observation #2880: loss = 0.0264932\n",
      "Train Observation #2890: loss = 0.283199\n",
      "Train Observation #2900: loss = 5.59028e-05\n",
      "Train Observation #2910: loss = 0.000383105\n",
      "Train Observation #2920: loss = 9.72386\n",
      "Train Observation #2930: loss = 0.140663\n",
      "Train Observation #2940: loss = 0.002911\n",
      "Train Observation #2950: loss = 0.00037255\n",
      "Train Observation #2960: loss = 5.84794e-06\n",
      "Train Observation #2970: loss = 0.00011335\n",
      "Train Observation #2980: loss = 0.00044128\n",
      "Train Observation #2990: loss = 0.00312478\n",
      "Train Observation #3000: loss = 0.00077273\n",
      "Train Observation #3010: loss = 8.20976\n",
      "Train Observation #3020: loss = 0.00219878\n",
      "Train Observation #3030: loss = 0.138283\n",
      "Train Observation #3040: loss = 3.02611e-06\n",
      "Train Observation #3050: loss = 0.000263373\n",
      "Train Observation #3060: loss = 0.00680473\n",
      "Train Observation #3070: loss = 0.153306\n",
      "Train Observation #3080: loss = 0.0781961\n",
      "Train Observation #3090: loss = 0.022312\n",
      "Train Observation #3100: loss = 1.74328\n",
      "Train Observation #3110: loss = 0.243245\n",
      "Train Observation #3120: loss = 0.000167159\n",
      "Train Observation #3130: loss = 0.285345\n",
      "Train Observation #3140: loss = 0.00208455\n",
      "Train Observation #3150: loss = 1.3505\n",
      "Train Observation #3160: loss = 0.00131129\n",
      "Train Observation #3170: loss = 0.00899045\n",
      "Train Observation #3180: loss = 4.97275\n",
      "Train Observation #3190: loss = 0.0119141\n",
      "Train Observation #3200: loss = 0.037265\n",
      "Train Observation #3210: loss = 2.13923\n",
      "Train Observation #3220: loss = 0.787549\n",
      "Train Observation #3230: loss = 0.000500125\n",
      "Train Observation #3240: loss = 0.511911\n",
      "Train Observation #3250: loss = 2.34732\n",
      "Train Observation #3260: loss = 0.182982\n",
      "Train Observation #3270: loss = 4.83707\n",
      "Train Observation #3280: loss = 0.0275492\n",
      "Train Observation #3290: loss = 1.29301\n",
      "Train Observation #3300: loss = 0.00728191\n",
      "Train Observation #3310: loss = 0.0418054\n",
      "Train Observation #3320: loss = 0.00141877\n",
      "Train Observation #3330: loss = 0.0627362\n",
      "Train Observation #3340: loss = 0.188614\n",
      "Train Observation #3350: loss = 8.03433\n",
      "Train Observation #3360: loss = 0.0484175\n",
      "Train Observation #3370: loss = 0.00316332\n",
      "Train Observation #3380: loss = 0.00139368\n",
      "Train Observation #3390: loss = 0.00136466\n",
      "Train Observation #3400: loss = 0.0205316\n",
      "Train Observation #3410: loss = 0.00381267\n",
      "Train Observation #3420: loss = 10.2749\n",
      "Train Observation #3430: loss = 0.0198611\n",
      "Train Observation #3440: loss = 1.46447\n",
      "Train Observation #3450: loss = 0.00333911\n",
      "Train Observation #3460: loss = 9.30501\n",
      "Train Observation #3470: loss = 0.281467\n",
      "Train Observation #3480: loss = 0.0642994\n",
      "Train Observation #3490: loss = 0.0189512\n",
      "Train Observation #3500: loss = 0.000385039\n",
      "Train Observation #3510: loss = 0.049626\n",
      "Train Observation #3520: loss = 0.643713\n",
      "Train Observation #3530: loss = 3.09365\n",
      "Train Observation #3540: loss = 0.144483\n",
      "Train Observation #3550: loss = 0.0190321\n",
      "Train Observation #3560: loss = 0.00164051\n",
      "Train Observation #3570: loss = 0.235653\n",
      "Train Observation #3580: loss = 0.00134415\n",
      "Train Observation #3590: loss = 0.000712977\n",
      "Train Observation #3600: loss = 6.16863\n",
      "Train Observation #3610: loss = 0.0159396\n",
      "Train Observation #3620: loss = 0.0626508\n",
      "Train Observation #3630: loss = 0.00764207\n",
      "Train Observation #3640: loss = 6.89238e-05\n",
      "Train Observation #3650: loss = 0.323208\n",
      "Train Observation #3660: loss = 4.34445\n",
      "Train Observation #3670: loss = 1.54613e-06\n",
      "Train Observation #3680: loss = 4.46576\n",
      "Train Observation #3690: loss = 0.00488964\n",
      "Train Observation #3700: loss = 0.0330319\n",
      "Train Observation #3710: loss = 3.01348\n",
      "Train Observation #3720: loss = 0.000809393\n",
      "Train Observation #3730: loss = 8.08424e-06\n",
      "Train Observation #3740: loss = 0.00333908\n",
      "Train Observation #3750: loss = 0.00345216\n",
      "Train Observation #3760: loss = 0.0016023\n",
      "Train Observation #3770: loss = 0.001245\n",
      "Train Observation #3780: loss = 0.000160555\n",
      "Train Observation #3790: loss = 0.0194462\n",
      "Train Observation #3800: loss = 0.0122984\n",
      "Train Observation #3810: loss = 0.0109193\n",
      "Train Observation #3820: loss = 0.0116708\n",
      "Train Observation #3830: loss = 0.000226609\n",
      "Train Observation #3840: loss = 6.9258e-05\n",
      "Train Observation #3850: loss = 0.218422\n",
      "Train Observation #3860: loss = 0.0011515\n",
      "Train Observation #3870: loss = 0.0127645\n",
      "Train Observation #3880: loss = 0.0160143\n",
      "Train Observation #3890: loss = 0.0310882\n",
      "Train Observation #3900: loss = 0.0021351\n",
      "Train Observation #3910: loss = 0.00724224\n",
      "Train Observation #3920: loss = 3.01251e-05\n",
      "Train Observation #3930: loss = 13.1332\n",
      "Train Observation #3940: loss = 0.0214433\n",
      "Train Observation #3950: loss = 0.10849\n",
      "Train Observation #3960: loss = 0.00683851\n",
      "Train Observation #3970: loss = 9.21081e-05\n",
      "Train Observation #3980: loss = 1.07163\n",
      "Train Observation #3990: loss = 0.000988035\n",
      "Train Observation #4000: loss = 0.00110241\n",
      "Train Observation #4010: loss = 0.000261217\n",
      "Train Observation #4020: loss = 0.000345157\n",
      "Train Observation #4030: loss = 7.63625\n",
      "Train Observation #4040: loss = 0.174904\n",
      "Train Observation #4050: loss = 8.35019e-05\n",
      "Train Observation #4060: loss = 0.0312442\n",
      "Train Observation #4070: loss = 2.75027\n",
      "Train Observation #4080: loss = 0.00229853\n",
      "Train Observation #4090: loss = 0.0312567\n",
      "Train Observation #4100: loss = 0.00300074\n",
      "Train Observation #4110: loss = 0.00462331\n",
      "Train Observation #4120: loss = 6.26057\n",
      "Train Observation #4130: loss = 0.0652171\n",
      "Train Observation #4140: loss = 0.0233354\n",
      "Train Observation #4150: loss = 0.108269\n",
      "Train Observation #4160: loss = 0.00642455\n",
      "Train Observation #4170: loss = 0.00100958\n",
      "Train Observation #4180: loss = 2.59581e-06\n",
      "Train Observation #4190: loss = 0.00639464\n",
      "Train Observation #4200: loss = 0.516955\n",
      "Train Observation #4210: loss = 0.00210175\n",
      "Train Observation #4220: loss = 0.283461\n",
      "Train Observation #4230: loss = 0.000560672\n",
      "Train Observation #4240: loss = 0.00464848\n",
      "Train Observation #4250: loss = 0.0114711\n",
      "Train Observation #4260: loss = 0.000676449\n",
      "Train Observation #4270: loss = 0.0015882\n",
      "Train Observation #4280: loss = 0.114469\n",
      "Train Observation #4290: loss = 0.00348632\n",
      "Train Observation #4300: loss = 0.000485605\n",
      "Train Observation #4310: loss = 0.000507789\n",
      "Train Observation #4320: loss = 0.00800808\n",
      "Train Observation #4330: loss = 1.52662e-05\n",
      "Train Observation #4340: loss = 0.000689579\n",
      "Train Observation #4350: loss = 0.00495301\n",
      "Train Observation #4360: loss = 0.000747253\n",
      "Train Observation #4370: loss = 6.11799e-05\n",
      "Train Observation #4380: loss = 4.4579\n",
      "Train Observation #4390: loss = 3.69133\n",
      "Train Observation #4400: loss = 1.75475e-06\n",
      "Train Observation #4410: loss = 6.10242\n",
      "Train Observation #4420: loss = 0.0499524\n",
      "Train Observation #4430: loss = 0.901629\n",
      "Train Observation #4440: loss = 0.00136334\n",
      "Train Observation #4450: loss = 0.00169447\n"
     ]
    }
   ],
   "source": [
    "for ix, t in enumerate(vocab_processor.fit_transform(texts_train)):\n",
    "    y_data = [[target_train[ix]]]\n",
    "    fd = {x_data: t, y_target:y_data}\n",
    "    \n",
    "    sess.run(train_step, feed_dict=fd)\n",
    "    temp_loss = sess.run(loss, feed_dict=fd)\n",
    "    losses.append(temp_loss)\n",
    "    \n",
    "    if (ix+1) % 10 == 0:\n",
    "        print('Train Observation #'+str(ix+1)+': loss = '+str(temp_loss))\n",
    "    \n",
    "    [[temp_pred]] = sess.run(predict, feed_dict=fd)\n",
    "    \n",
    "    train_acc_temp = target_train[ix] == np.round(temp_pred)\n",
    "    train_acc.append(train_acc_temp)\n",
    "    if len(train_acc) >= 50:\n",
    "        train_acc_ave.append(np.mean(train_acc[-50:]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Observation #50\n",
      "Test Observation #100\n",
      "Test Observation #150\n",
      "Test Observation #200\n",
      "Test Observation #250\n",
      "Test Observation #300\n",
      "Test Observation #350\n",
      "Test Observation #400\n",
      "Test Observation #450\n",
      "Test Observation #500\n",
      "Test Observation #550\n",
      "Test Observation #600\n",
      "Test Observation #650\n",
      "Test Observation #700\n",
      "Test Observation #750\n",
      "Test Observation #800\n",
      "Test Observation #850\n",
      "Test Observation #900\n",
      "Test Observation #950\n",
      "Test Observation #1000\n",
      "Test Observation #1050\n",
      "Test Observation #1100\n",
      "\n",
      "Overall Test Accuracy: 0.7847533632286996\n"
     ]
    }
   ],
   "source": [
    "for ix, t in enumerate(vocab_processor.fit_transform(texts_test)):\n",
    "    y_data = [[target_test[ix]]]\n",
    "    fd = {x_data: t, y_target: y_data}\n",
    "    \n",
    "    if (ix + 1) % 50 == 0:\n",
    "        print('Test Observation #{}'.format(ix + 1))\n",
    "        \n",
    "    [[temp_pred]] = sess.run(predict, feed_dict=fd)\n",
    "    \n",
    "    test_acc_temp = target_test[ix] == np.round(temp_pred)\n",
    "    test_acc.append(test_acc_temp)\n",
    "    \n",
    "print()\n",
    "print('Overall Test Accuracy: {}'.format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
