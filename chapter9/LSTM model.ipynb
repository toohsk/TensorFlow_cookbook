{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "min_word_freq = 5\n",
    "rnn_size = 128\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_seq_len = 50\n",
    "embedding_size = rnn_size\n",
    "save_every = 500\n",
    "eval_every = 50\n",
    "\n",
    "prime_texts = ['thou art more', 'to be or not to', 'wherefore art thou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テキスト　クレンジング\n",
    "data_dir = 'temp'\n",
    "data_file = 'shakespeare.txt'\n",
    "model_path = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)\n",
    "\n",
    "# ハイフンとアポストロフィ以外の句読点を削除\n",
    "punctuation = string.punctuation\n",
    "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loding Shakespeare data.\n"
     ]
    }
   ],
   "source": [
    "# モデルフォルダを作成\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "    \n",
    "# データフォルダを作成\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "print(\"Loding Shakespeare data.\")\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Not found, downloading Shakespeare texts from www.gutenberg.org')\n",
    "    \n",
    "    shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    s_text = s_text[7675:]\n",
    "    s_text = s_text.replace('\\r\\n', '')\n",
    "    s_text = s_text.replace('\\n', '')\n",
    "    \n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "        \n",
    "else:\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語彙の辞書を作成する\n",
    "def build_vocab(text, min_word_freq):\n",
    "    word_counts = collections.Counter(text.split(' '))\n",
    "    word_counts = {key:val for key, val in word_counts.items() if val > min_word_freq}\n",
    "    \n",
    "    words = word_counts.keys()\n",
    "    vocab_to_ix_dict = {key:(ix+1) for ix, key in enumerate(words)}\n",
    "    vocab_to_ix_dict['unknown'] = 0\n",
    "    \n",
    "    ix_to_vocav_dict = {val:key for key, val in vocab_to_ix_dict.items()}\n",
    "    \n",
    "    return (ix_to_vocav_dict, vocab_to_ix_dict)\n",
    "\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\n",
    "vocab_size = len(ix2vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストデータを単語ベクトルに変換する\n",
    "s_text_words = s_text.split(' ')\n",
    "s_text_ix = []\n",
    "\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except:\n",
    "        s_text_ix.append(0)\n",
    "        \n",
    "s_text_ix = np.array(s_text_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model():\n",
    "    \n",
    "    def __init__(self, rnn_size, batch_size, learning_rate, seq_len, vocab_size, infer_sample=False):\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.seq_len = seq_len\n",
    "            \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size],\n",
    "                                tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size],\n",
    "                                tf.float32, tf.constant_initializer(0.0))\n",
    "            \n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.rnn_size],\n",
    "                                            tf.float32,\n",
    "                                            tf.random_normal_initializer())\n",
    "            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            \n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.seq_len,\n",
    "                                  value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "            \n",
    "        # テキスト生成中はループ関数を追加\n",
    "        # i番目の出力から i+1 番目の入力を取得する方法を定義\n",
    "        def inferred_loop(prev, count):\n",
    "            # 隠れそう\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return (output)\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(\n",
    "            rnn_inputs_trimmed,\n",
    "            self.initial_state,\n",
    "            self.lstm_cell,\n",
    "            loop_function=inferred_loop if infer_sample else None)\n",
    "        \n",
    "        # 推測されていない出力\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "        \n",
    "        # ロジットと出力\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        # 損失関数\n",
    "        loss_func = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_func(\n",
    "            [self.logit_output],\n",
    "            [tf.reshape(self.y_output, [-1])],\n",
    "            [tf.ones([self.batch_size * self.seq_len])],\n",
    "            self.vocab_size)\n",
    "        \n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "        \n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=10, prime_text='thou art'):\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        \n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state: state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "            \n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state: state}\n",
    "            [model_output, state] = sess.run(\n",
    "                [self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            \n",
    "            if sample == 0:\n",
    "                break\n",
    "                \n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "            \n",
    "        return (out_sentence)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_Model(rnn_size=rnn_size, batch_size=batch_size, learning_rate=learning_rate, seq_len=training_seq_len, vocab_size=vocab_size, infer_sample=False)\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_lstm_model = LSTM_Model(rnn_size=rnn_size, batch_size=batch_size, learning_rate=learning_rate,\n",
    "                        seq_len=training_seq_len, vocab_size=vocab_size, infer_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "num_batches = int(len(s_text_ix) / (batch_size * training_seq_len)) + 1\n",
    "\n",
    "batches = np.array_split(s_text_ix, num_batches)\n",
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]\n",
    "\n",
    "# 全ての変数を初期化\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/10...\n",
      "Iteration: 10, Epoch: 1, Batch: 10 out of 182,                   Loss: 9.90\n",
      "Iteration: 20, Epoch: 1, Batch: 20 out of 182,                   Loss: 9.26\n",
      "Iteration: 30, Epoch: 1, Batch: 30 out of 182,                   Loss: 8.73\n",
      "Iteration: 40, Epoch: 1, Batch: 40 out of 182,                   Loss: 8.39\n",
      "Iteration: 50, Epoch: 1, Batch: 50 out of 182,                   Loss: 8.07\n",
      "thou art more think saturn make i have to to the\n",
      "to be or not to to the\n",
      "wherefore art thou\n",
      "Iteration: 60, Epoch: 1, Batch: 60 out of 182,                   Loss: 7.82\n",
      "Iteration: 70, Epoch: 1, Batch: 70 out of 182,                   Loss: 7.58\n",
      "Iteration: 80, Epoch: 1, Batch: 80 out of 182,                   Loss: 7.50\n",
      "Iteration: 90, Epoch: 1, Batch: 90 out of 182,                   Loss: 7.39\n",
      "Iteration: 100, Epoch: 1, Batch: 100 out of 182,                   Loss: 7.11\n",
      "thou art more think the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast to the\n",
      "Iteration: 110, Epoch: 1, Batch: 110 out of 182,                   Loss: 6.98\n",
      "Iteration: 120, Epoch: 1, Batch: 120 out of 182,                   Loss: 6.83\n",
      "Iteration: 130, Epoch: 1, Batch: 130 out of 182,                   Loss: 6.81\n",
      "Iteration: 140, Epoch: 1, Batch: 140 out of 182,                   Loss: 6.71\n",
      "Iteration: 150, Epoch: 1, Batch: 150 out of 182,                   Loss: 6.60\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou art thou art thou art thou\n",
      "Iteration: 160, Epoch: 1, Batch: 160 out of 182,                   Loss: 6.59\n",
      "Iteration: 170, Epoch: 1, Batch: 170 out of 182,                   Loss: 6.30\n",
      "Iteration: 180, Epoch: 1, Batch: 180 out of 182,                   Loss: 6.51\n",
      "Starting Epoch 2/10...\n",
      "Iteration: 190, Epoch: 2, Batch: 9 out of 182,                   Loss: 6.98\n",
      "Iteration: 200, Epoch: 2, Batch: 19 out of 182,                   Loss: 6.99\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou art thou art thou art thou\n",
      "Iteration: 210, Epoch: 2, Batch: 29 out of 182,                   Loss: 6.57\n",
      "Iteration: 220, Epoch: 2, Batch: 39 out of 182,                   Loss: 6.48\n",
      "Iteration: 230, Epoch: 2, Batch: 49 out of 182,                   Loss: 6.31\n",
      "Iteration: 240, Epoch: 2, Batch: 59 out of 182,                   Loss: 6.45\n",
      "Iteration: 250, Epoch: 2, Batch: 69 out of 182,                   Loss: 6.47\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou art thou\n",
      "Iteration: 260, Epoch: 2, Batch: 79 out of 182,                   Loss: 6.46\n",
      "Iteration: 270, Epoch: 2, Batch: 89 out of 182,                   Loss: 6.57\n",
      "Iteration: 280, Epoch: 2, Batch: 99 out of 182,                   Loss: 6.39\n",
      "Iteration: 290, Epoch: 2, Batch: 109 out of 182,                   Loss: 6.47\n",
      "Iteration: 300, Epoch: 2, Batch: 119 out of 182,                   Loss: 6.40\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou art thou art thou\n",
      "Iteration: 310, Epoch: 2, Batch: 129 out of 182,                   Loss: 6.40\n",
      "Iteration: 320, Epoch: 2, Batch: 139 out of 182,                   Loss: 5.96\n",
      "Iteration: 330, Epoch: 2, Batch: 149 out of 182,                   Loss: 6.31\n",
      "Iteration: 340, Epoch: 2, Batch: 159 out of 182,                   Loss: 6.29\n",
      "Iteration: 350, Epoch: 2, Batch: 169 out of 182,                   Loss: 6.45\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou art thou hast thou\n",
      "Iteration: 360, Epoch: 2, Batch: 179 out of 182,                   Loss: 6.25\n",
      "Starting Epoch 3/10...\n",
      "Iteration: 370, Epoch: 3, Batch: 8 out of 182,                   Loss: 6.13\n",
      "Iteration: 380, Epoch: 3, Batch: 18 out of 182,                   Loss: 6.16\n",
      "Iteration: 390, Epoch: 3, Batch: 28 out of 182,                   Loss: 5.98\n",
      "Iteration: 400, Epoch: 3, Batch: 38 out of 182,                   Loss: 6.27\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou art thou\n",
      "Iteration: 410, Epoch: 3, Batch: 48 out of 182,                   Loss: 6.24\n",
      "Iteration: 420, Epoch: 3, Batch: 58 out of 182,                   Loss: 5.93\n",
      "Iteration: 430, Epoch: 3, Batch: 68 out of 182,                   Loss: 6.15\n",
      "Iteration: 440, Epoch: 3, Batch: 78 out of 182,                   Loss: 6.28\n",
      "Iteration: 450, Epoch: 3, Batch: 88 out of 182,                   Loss: 6.34\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou art thou art thou art thou\n",
      "Iteration: 460, Epoch: 3, Batch: 98 out of 182,                   Loss: 6.03\n",
      "Iteration: 470, Epoch: 3, Batch: 108 out of 182,                   Loss: 6.25\n",
      "Iteration: 480, Epoch: 3, Batch: 118 out of 182,                   Loss: 6.22\n",
      "Iteration: 490, Epoch: 3, Batch: 128 out of 182,                   Loss: 6.26\n",
      "Iteration: 500, Epoch: 3, Batch: 138 out of 182,                   Loss: 6.31\n",
      "Model saved to: temp/shakespeare_model/model\n",
      "thou art more than they are not distributed or usedcommercially prohibited commercial distribution\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou hast thou\n",
      "Iteration: 510, Epoch: 3, Batch: 148 out of 182,                   Loss: 6.08\n",
      "Iteration: 520, Epoch: 3, Batch: 158 out of 182,                   Loss: 6.11\n",
      "Iteration: 530, Epoch: 3, Batch: 168 out of 182,                   Loss: 6.31\n",
      "Iteration: 540, Epoch: 3, Batch: 178 out of 182,                   Loss: 6.06\n",
      "Starting Epoch 4/10...\n",
      "Iteration: 550, Epoch: 4, Batch: 7 out of 182,                   Loss: 6.32\n",
      "thou art more than they are not distributed or usedcommercially prohibited commercial distribution\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou art thou\n",
      "Iteration: 560, Epoch: 4, Batch: 17 out of 182,                   Loss: 6.26\n",
      "Iteration: 570, Epoch: 4, Batch: 27 out of 182,                   Loss: 6.20\n",
      "Iteration: 580, Epoch: 4, Batch: 37 out of 182,                   Loss: 6.24\n",
      "Iteration: 590, Epoch: 4, Batch: 47 out of 182,                   Loss: 5.83\n",
      "Iteration: 600, Epoch: 4, Batch: 57 out of 182,                   Loss: 6.16\n",
      "thou art more than they were\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thy thy\n",
      "Iteration: 610, Epoch: 4, Batch: 67 out of 182,                   Loss: 6.09\n",
      "Iteration: 620, Epoch: 4, Batch: 77 out of 182,                   Loss: 6.13\n",
      "Iteration: 630, Epoch: 4, Batch: 87 out of 182,                   Loss: 6.33\n",
      "Iteration: 640, Epoch: 4, Batch: 97 out of 182,                   Loss: 5.85\n",
      "Iteration: 650, Epoch: 4, Batch: 107 out of 182,                   Loss: 6.05\n",
      "thou art more than they are not so much as i am not\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thy thy\n",
      "Iteration: 660, Epoch: 4, Batch: 117 out of 182,                   Loss: 6.08\n",
      "Iteration: 670, Epoch: 4, Batch: 127 out of 182,                   Loss: 6.00\n",
      "Iteration: 680, Epoch: 4, Batch: 137 out of 182,                   Loss: 6.27\n",
      "Iteration: 690, Epoch: 4, Batch: 147 out of 182,                   Loss: 5.99\n",
      "Iteration: 700, Epoch: 4, Batch: 157 out of 182,                   Loss: 6.16\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou art thou\n",
      "Iteration: 710, Epoch: 4, Batch: 167 out of 182,                   Loss: 6.27\n",
      "Iteration: 720, Epoch: 4, Batch: 177 out of 182,                   Loss: 6.07\n",
      "Starting Epoch 5/10...\n",
      "Iteration: 730, Epoch: 5, Batch: 6 out of 182,                   Loss: 5.98\n",
      "Iteration: 740, Epoch: 5, Batch: 16 out of 182,                   Loss: 5.93\n",
      "Iteration: 750, Epoch: 5, Batch: 26 out of 182,                   Loss: 6.05\n",
      "thou art more than they are not distributed or usedcommercially prohibited commercial distribution\n",
      "to be or not to the\n",
      "wherefore art thou hast thou shalt thy\n",
      "Iteration: 760, Epoch: 5, Batch: 36 out of 182,                   Loss: 6.03\n",
      "Iteration: 770, Epoch: 5, Batch: 46 out of 182,                   Loss: 6.01\n",
      "Iteration: 780, Epoch: 5, Batch: 56 out of 182,                   Loss: 5.79\n",
      "Iteration: 790, Epoch: 5, Batch: 66 out of 182,                   Loss: 6.10\n",
      "Iteration: 800, Epoch: 5, Batch: 76 out of 182,                   Loss: 6.00\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thy\n",
      "Iteration: 810, Epoch: 5, Batch: 86 out of 182,                   Loss: 5.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 820, Epoch: 5, Batch: 96 out of 182,                   Loss: 6.01\n",
      "Iteration: 830, Epoch: 5, Batch: 106 out of 182,                   Loss: 6.01\n",
      "Iteration: 840, Epoch: 5, Batch: 116 out of 182,                   Loss: 5.73\n",
      "Iteration: 850, Epoch: 5, Batch: 126 out of 182,                   Loss: 5.74\n",
      "thou art more than they are not so much as i am a\n",
      "to be or not to the\n",
      "wherefore art thou hast thou shalt not so\n",
      "Iteration: 860, Epoch: 5, Batch: 136 out of 182,                   Loss: 6.01\n",
      "Iteration: 870, Epoch: 5, Batch: 146 out of 182,                   Loss: 6.07\n",
      "Iteration: 880, Epoch: 5, Batch: 156 out of 182,                   Loss: 5.87\n",
      "Iteration: 890, Epoch: 5, Batch: 166 out of 182,                   Loss: 6.08\n",
      "Iteration: 900, Epoch: 5, Batch: 176 out of 182,                   Loss: 5.95\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou shalt not be\n",
      "Starting Epoch 6/10...\n",
      "Iteration: 910, Epoch: 6, Batch: 5 out of 182,                   Loss: 5.86\n",
      "Iteration: 920, Epoch: 6, Batch: 15 out of 182,                   Loss: 6.02\n",
      "Iteration: 930, Epoch: 6, Batch: 25 out of 182,                   Loss: 6.10\n",
      "Iteration: 940, Epoch: 6, Batch: 35 out of 182,                   Loss: 6.05\n",
      "Iteration: 950, Epoch: 6, Batch: 45 out of 182,                   Loss: 6.05\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou hast thou shalt not so\n",
      "Iteration: 960, Epoch: 6, Batch: 55 out of 182,                   Loss: 5.95\n",
      "Iteration: 970, Epoch: 6, Batch: 65 out of 182,                   Loss: 5.99\n",
      "Iteration: 980, Epoch: 6, Batch: 75 out of 182,                   Loss: 5.90\n",
      "Iteration: 990, Epoch: 6, Batch: 85 out of 182,                   Loss: 5.97\n",
      "Iteration: 1000, Epoch: 6, Batch: 95 out of 182,                   Loss: 5.83\n",
      "Model saved to: temp/shakespeare_model/model\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thy\n",
      "Iteration: 1010, Epoch: 6, Batch: 105 out of 182,                   Loss: 5.97\n",
      "Iteration: 1020, Epoch: 6, Batch: 115 out of 182,                   Loss: 5.72\n",
      "Iteration: 1030, Epoch: 6, Batch: 125 out of 182,                   Loss: 5.95\n",
      "Iteration: 1040, Epoch: 6, Batch: 135 out of 182,                   Loss: 6.13\n",
      "Iteration: 1050, Epoch: 6, Batch: 145 out of 182,                   Loss: 6.02\n",
      "thou art more than they were\n",
      "to be or not to the\n",
      "wherefore art thou shalt not so\n",
      "Iteration: 1060, Epoch: 6, Batch: 155 out of 182,                   Loss: 5.97\n",
      "Iteration: 1070, Epoch: 6, Batch: 165 out of 182,                   Loss: 5.89\n",
      "Iteration: 1080, Epoch: 6, Batch: 175 out of 182,                   Loss: 5.89\n",
      "Starting Epoch 7/10...\n",
      "Iteration: 1090, Epoch: 7, Batch: 4 out of 182,                   Loss: 6.08\n",
      "Iteration: 1100, Epoch: 7, Batch: 14 out of 182,                   Loss: 5.90\n",
      "thou art more than than they were not in the\n",
      "to be or not to the\n",
      "wherefore art thou shalt not so\n",
      "Iteration: 1110, Epoch: 7, Batch: 24 out of 182,                   Loss: 6.06\n",
      "Iteration: 1120, Epoch: 7, Batch: 34 out of 182,                   Loss: 5.83\n",
      "Iteration: 1130, Epoch: 7, Batch: 44 out of 182,                   Loss: 5.90\n",
      "Iteration: 1140, Epoch: 7, Batch: 54 out of 182,                   Loss: 5.92\n",
      "Iteration: 1150, Epoch: 7, Batch: 64 out of 182,                   Loss: 5.95\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou hast thou shalt not so\n",
      "Iteration: 1160, Epoch: 7, Batch: 74 out of 182,                   Loss: 5.88\n",
      "Iteration: 1170, Epoch: 7, Batch: 84 out of 182,                   Loss: 6.02\n",
      "Iteration: 1180, Epoch: 7, Batch: 94 out of 182,                   Loss: 5.85\n",
      "Iteration: 1190, Epoch: 7, Batch: 104 out of 182,                   Loss: 5.97\n",
      "Iteration: 1200, Epoch: 7, Batch: 114 out of 182,                   Loss: 5.74\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou shalt not\n",
      "Iteration: 1210, Epoch: 7, Batch: 124 out of 182,                   Loss: 6.19\n",
      "Iteration: 1220, Epoch: 7, Batch: 134 out of 182,                   Loss: 5.58\n",
      "Iteration: 1230, Epoch: 7, Batch: 144 out of 182,                   Loss: 6.05\n",
      "Iteration: 1240, Epoch: 7, Batch: 154 out of 182,                   Loss: 5.72\n",
      "Iteration: 1250, Epoch: 7, Batch: 164 out of 182,                   Loss: 5.91\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou shalt not so\n",
      "Iteration: 1260, Epoch: 7, Batch: 174 out of 182,                   Loss: 5.88\n",
      "Starting Epoch 8/10...\n",
      "Iteration: 1270, Epoch: 8, Batch: 3 out of 182,                   Loss: 5.87\n",
      "Iteration: 1280, Epoch: 8, Batch: 13 out of 182,                   Loss: 6.02\n",
      "Iteration: 1290, Epoch: 8, Batch: 23 out of 182,                   Loss: 5.86\n",
      "Iteration: 1300, Epoch: 8, Batch: 33 out of 182,                   Loss: 5.79\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou shalt not so\n",
      "Iteration: 1310, Epoch: 8, Batch: 43 out of 182,                   Loss: 5.77\n",
      "Iteration: 1320, Epoch: 8, Batch: 53 out of 182,                   Loss: 5.81\n",
      "Iteration: 1330, Epoch: 8, Batch: 63 out of 182,                   Loss: 5.84\n",
      "Iteration: 1340, Epoch: 8, Batch: 73 out of 182,                   Loss: 6.13\n",
      "Iteration: 1350, Epoch: 8, Batch: 83 out of 182,                   Loss: 5.91\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou hast thou shalt not so\n",
      "Iteration: 1360, Epoch: 8, Batch: 93 out of 182,                   Loss: 5.73\n",
      "Iteration: 1370, Epoch: 8, Batch: 103 out of 182,                   Loss: 5.97\n",
      "Iteration: 1380, Epoch: 8, Batch: 113 out of 182,                   Loss: 5.66\n",
      "Iteration: 1390, Epoch: 8, Batch: 123 out of 182,                   Loss: 5.68\n",
      "Iteration: 1400, Epoch: 8, Batch: 133 out of 182,                   Loss: 5.73\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou shalt not so\n",
      "Iteration: 1410, Epoch: 8, Batch: 143 out of 182,                   Loss: 5.63\n",
      "Iteration: 1420, Epoch: 8, Batch: 153 out of 182,                   Loss: 5.82\n",
      "Iteration: 1430, Epoch: 8, Batch: 163 out of 182,                   Loss: 5.86\n",
      "Iteration: 1440, Epoch: 8, Batch: 173 out of 182,                   Loss: 5.97\n",
      "Starting Epoch 9/10...\n",
      "Iteration: 1450, Epoch: 9, Batch: 2 out of 182,                   Loss: 5.83\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou shalt not be so\n",
      "Iteration: 1460, Epoch: 9, Batch: 12 out of 182,                   Loss: 5.83\n",
      "Iteration: 1470, Epoch: 9, Batch: 22 out of 182,                   Loss: 5.54\n",
      "Iteration: 1480, Epoch: 9, Batch: 32 out of 182,                   Loss: 5.82\n",
      "Iteration: 1490, Epoch: 9, Batch: 42 out of 182,                   Loss: 5.93\n",
      "Iteration: 1500, Epoch: 9, Batch: 52 out of 182,                   Loss: 5.80\n",
      "Model saved to: temp/shakespeare_model/model\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou shalt not be\n",
      "Iteration: 1510, Epoch: 9, Batch: 62 out of 182,                   Loss: 5.94\n",
      "Iteration: 1520, Epoch: 9, Batch: 72 out of 182,                   Loss: 5.83\n",
      "Iteration: 1530, Epoch: 9, Batch: 82 out of 182,                   Loss: 5.78\n",
      "Iteration: 1540, Epoch: 9, Batch: 92 out of 182,                   Loss: 5.47\n",
      "Iteration: 1550, Epoch: 9, Batch: 102 out of 182,                   Loss: 5.77\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1560, Epoch: 9, Batch: 112 out of 182,                   Loss: 5.99\n",
      "Iteration: 1570, Epoch: 9, Batch: 122 out of 182,                   Loss: 5.87\n",
      "Iteration: 1580, Epoch: 9, Batch: 132 out of 182,                   Loss: 5.84\n",
      "Iteration: 1590, Epoch: 9, Batch: 142 out of 182,                   Loss: 5.74\n",
      "Iteration: 1600, Epoch: 9, Batch: 152 out of 182,                   Loss: 5.88\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou shalt not be a\n",
      "Iteration: 1610, Epoch: 9, Batch: 162 out of 182,                   Loss: 5.82\n",
      "Iteration: 1620, Epoch: 9, Batch: 172 out of 182,                   Loss: 5.43\n",
      "Starting Epoch 10/10...\n",
      "Iteration: 1630, Epoch: 10, Batch: 1 out of 182,                   Loss: 5.49\n",
      "Iteration: 1640, Epoch: 10, Batch: 11 out of 182,                   Loss: 5.77\n",
      "Iteration: 1650, Epoch: 10, Batch: 21 out of 182,                   Loss: 5.76\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou shalt not be\n",
      "Iteration: 1660, Epoch: 10, Batch: 31 out of 182,                   Loss: 5.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1670, Epoch: 10, Batch: 41 out of 182,                   Loss: 5.69\n",
      "Iteration: 1680, Epoch: 10, Batch: 51 out of 182,                   Loss: 5.81\n",
      "Iteration: 1690, Epoch: 10, Batch: 61 out of 182,                   Loss: 5.81\n",
      "Iteration: 1700, Epoch: 10, Batch: 71 out of 182,                   Loss: 5.90\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou shalt not be\n",
      "Iteration: 1710, Epoch: 10, Batch: 81 out of 182,                   Loss: 5.94\n",
      "Iteration: 1720, Epoch: 10, Batch: 91 out of 182,                   Loss: 5.72\n",
      "Iteration: 1730, Epoch: 10, Batch: 101 out of 182,                   Loss: 5.79\n",
      "Iteration: 1740, Epoch: 10, Batch: 111 out of 182,                   Loss: 5.69\n",
      "Iteration: 1750, Epoch: 10, Batch: 121 out of 182,                   Loss: 5.72\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1760, Epoch: 10, Batch: 131 out of 182,                   Loss: 5.86\n",
      "Iteration: 1770, Epoch: 10, Batch: 141 out of 182,                   Loss: 5.90\n",
      "Iteration: 1780, Epoch: 10, Batch: 151 out of 182,                   Loss: 5.76\n",
      "Iteration: 1790, Epoch: 10, Batch: 161 out of 182,                   Loss: 5.72\n",
      "Iteration: 1800, Epoch: 10, Batch: 171 out of 182,                   Loss: 5.88\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou shalt not be\n",
      "Iteration: 1810, Epoch: 10, Batch: 181 out of 182,                   Loss: 5.80\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "iteration_count = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(batches)\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    print('Starting Epoch {}/{}...'.format(epoch+1, epochs))\n",
    "    \n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch,\n",
    "                         lstm_model.y_output: targets[ix]}\n",
    "        c, h = lstm_model.initial_state\n",
    "        training_dict[c] = state.c\n",
    "        training_dict[h] = state.h\n",
    "        \n",
    "        temp_loss, state, _ = sess.run(\n",
    "            [lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n",
    "            feed_dict=training_dict)\n",
    "        train_loss.append(temp_loss)\n",
    "        \n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, \\\n",
    "                  Loss: {:.2f}'.format(*summary_nums))\n",
    "            \n",
    "        if iteration_count % save_every == 0:\n",
    "            model_file_name = os.path.join(full_model_dir, 'model')\n",
    "            saver.save(sess, model_file_name, global_step=iteration_count)\n",
    "            print('Model saved to: {}'.format(model_file_name))\n",
    "            \n",
    "            dictionary_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "            with open(dictionary_file, 'wb') as dict_file_conn:\n",
    "                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n",
    "                \n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "                \n",
    "        iteration_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIqI4x0RARBTnmAiIiOLc/wc2y66oO7VYmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1257dc860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, 'k-')\n",
    "plt.title('Seq2Seq loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
